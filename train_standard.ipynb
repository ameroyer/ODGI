{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2, 3\"\n",
    "from math import ceil\n",
    "\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.training.summary_io import SummaryWriterCache\n",
    "\n",
    "import graph_manager\n",
    "import net\n",
    "import eval_utils\n",
    "import loss_utils\n",
    "import tf_inputs\n",
    "import tf_utils\n",
    "import viz\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39688 training steps\n",
      "...which means 121 epochs\n",
      "10476 training samples (328 iters)\n",
      "2619 validation samples (82 iters)\n",
      "\n",
      "\u001b[41mConfig:\u001b[0m\n",
      "\u001b[96mbatch_size:\u001b[0m 16\n",
      "\u001b[96mcenters_localization_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mconfidence_loss_weight:\u001b[0m 5.0\n",
      "\u001b[96mdata_classes:\u001b[0m ['Biker', 'Bus', 'Car', 'Cart', 'Pedestrian', 'Skater']\n",
      "\u001b[96mexp_name:\u001b[0m sdd\n",
      "\u001b[96mfeature_keys:\u001b[0m ['im_id', 'num_boxes', 'bounding_boxes', 'classes']\n",
      "\u001b[96mgpu_mem_frac:\u001b[0m 1.0\n",
      "\u001b[96mimage_folder:\u001b[0m /home/aroyer/Datasets/sdd_images\n",
      "\u001b[96mlast_test_batch_size:\u001b[0m 27\n",
      "\u001b[96mlearning_rate:\u001b[0m 0.0005\n",
      "\u001b[96mnoobj_confidence_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mnum_classes:\u001b[0m 6\n",
      "\u001b[96mnum_epochs:\u001b[0m 120\n",
      "\u001b[96mnum_gpus:\u001b[0m 2\n",
      "\u001b[96mnum_steps:\u001b[0m 39688\n",
      "\u001b[96moffsets_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mretrieval_intersection_threshold:\u001b[0m [0.25, 0.5, 0.75]\n",
      "\u001b[96msave_checkpoint_secs:\u001b[0m 3600\n",
      "\u001b[96msave_evaluation_steps:\u001b[0m 500\n",
      "\u001b[96msave_summaries_steps:\u001b[0m 200\n",
      "\u001b[96mscales_localization_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96msetting:\u001b[0m sdd\n",
      "\u001b[96mshuffle_buffer:\u001b[0m 2000\n",
      "\u001b[96msubset:\u001b[0m -1\n",
      "\u001b[96mtest_batch_size:\u001b[0m 16\n",
      "\u001b[96mtest_max_num_bbs:\u001b[0m 100\n",
      "\u001b[96mtest_num_iters_per_epoch:\u001b[0m 82\n",
      "\u001b[96mtest_num_samples:\u001b[0m 2619\n",
      "\u001b[96mtest_num_samples_per_iter:\u001b[0m 32\n",
      "\u001b[96mtest_tfrecords:\u001b[0m Data/sdd_test\n",
      "\u001b[96mtrain_max_num_bbs:\u001b[0m 100\n",
      "\u001b[96mtrain_num_iters_per_epoch:\u001b[0m 328\n",
      "\u001b[96mtrain_num_samples:\u001b[0m 10476\n",
      "\u001b[96mtrain_num_samples_per_iter:\u001b[0m 32\n",
      "\u001b[96mtrain_tfrecords:\u001b[0m Data/sdd_train\n"
     ]
    }
   ],
   "source": [
    "data = 'vedai'\n",
    "\n",
    "configuration = {}\n",
    "if data == 'vedai':\n",
    "    configuration['setting'] = 'vedai'\n",
    "    configuration['exp_name'] = 'vedai'\n",
    "    configuration['save_summaries_steps'] = 100\n",
    "    configuration['save_evaluation_steps'] = 250\n",
    "    configuration['num_epochs'] = 1000\n",
    "elif data == 'stanford':\n",
    "    configuration['setting'] = 'sdd'\n",
    "    configuration['exp_name'] = 'sdd'\n",
    "    configuration['save_summaries_steps'] = 200\n",
    "    configuration['save_evaluation_steps'] = 500\n",
    "    configuration['num_epochs'] = 120\n",
    "    \n",
    "## Metadata\n",
    "tfrecords_path = '/home/aroyer/indolentDetect/Data/metadata_%s.txt'\n",
    "metadata = graph_manager.load_metadata(tfrecords_path % configuration['setting'])\n",
    "configuration.update(metadata)\n",
    "configuration['num_classes'] = len(configuration['data_classes'])\n",
    "\n",
    "## GPUs\n",
    "configuration['num_gpus'] = 2                                 \n",
    "configuration['gpu_mem_frac'] = 1.\n",
    "\n",
    "## Inputs Pipeline\n",
    "configuration['subset'] = -1\n",
    "configuration['batch_size'] = 16\n",
    "configuration['test_batch_size'] = 16\n",
    "configuration['shuffle_buffer'] = 2000\n",
    "    \n",
    "## Training\n",
    "configuration['learning_rate'] = 1e-3\n",
    "configuration['centers_localization_loss_weight'] = 1.\n",
    "configuration['scales_localization_loss_weight']  = 1.\n",
    "configuration['confidence_loss_weight']  = 5.\n",
    "configuration['noobj_confidence_loss_weight']  = 1.\n",
    "configuration['offsets_loss_weight']  = 1.\n",
    "\n",
    "## Evaluation\n",
    "configuration['save_checkpoint_secs'] = 3600\n",
    "configuration['retrieval_intersection_threshold'] = [0.25, 0.5, 0.75]\n",
    "\n",
    "graph_manager.finalize_configuration(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, \n",
    "                 outputs, \n",
    "                 configuration,\n",
    "                 is_training=True,\n",
    "                 reuse=False, \n",
    "                 verbose=False,\n",
    "                 scope_name='model'):\n",
    "    \"\"\"Forward-pass in the net\"\"\"\n",
    "    with tf.variable_scope(scope_name, reuse=reuse):\n",
    "        activations = net.tiny_yolo_v2(\n",
    "            inputs[\"image\"], is_training=is_training, reuse=reuse, verbose=verbose, **configuration)\n",
    "        net.get_detection_outputs(activations, outputs, reuse=reuse, verbose=verbose, **configuration)\n",
    "            \n",
    "            \n",
    "def train_pass(inputs, configuration, is_chief=False):\n",
    "    \"\"\" Compute outputs of the net and add losses to the graph\"\"\"\n",
    "    outputs = {}\n",
    "    base_name = graph_manager.get_defaults(configuration, ['base_name'], verbose=is_chief)[0]\n",
    "    if is_chief: print(' \\033[34m%s:\\033[0m' % base_name)\n",
    "        \n",
    "    # Feed forward\n",
    "    with tf.name_scope('%s/net' % base_name):\n",
    "        forward_pass(inputs, outputs, configuration, scope_name=base_name, \n",
    "                     is_training=True, reuse=not is_chief, verbose=is_chief) \n",
    "        \n",
    "    # Add losses\n",
    "    with tf.name_scope('%s/loss' % base_name):\n",
    "        graph_manager.add_losses_to_graph(\n",
    "            loss_utils.get_standard_loss, inputs, outputs, configuration, is_chief=is_chief, verbose=is_chief)\n",
    "        \n",
    "    if is_chief:\n",
    "        print('\\n'.join(\"    \\033[32m%s\\033[0m: shape=%s, dtype=%s\" % (\n",
    "            key, value.get_shape().as_list(), value.dtype) for key, value in outputs.items()))\n",
    "    return outputs\n",
    "        \n",
    "    \n",
    "def eval_pass(inputs, configuration, metrics_to_norms, clear_metrics_op, update_metrics_op, \n",
    "              device=0, is_chief=False):\n",
    "    \"\"\" Compute output of the net and add metrics update and reset operatiosn to the graph\"\"\"\n",
    "    outputs = {}\n",
    "    base_name = graph_manager.get_defaults(configuration, ['base_name'], verbose=is_chief)[0]\n",
    "    if is_chief: print(' \\033[34m%s:\\033[0m' % base_name)\n",
    "        \n",
    "    # Feed forward\n",
    "    with tf.name_scope('%s/net' % base_name):\n",
    "        forward_pass(inputs, outputs, configuration, scope_name=base_name, is_training=False, \n",
    "                     reuse=True, verbose=is_chief) \n",
    "        \n",
    "    with tf.name_scope('%s/eval' % base_name):\n",
    "        # Add number of samples counter\n",
    "        graph_manager.add_metrics_to_graph(\n",
    "            eval_utils.get_samples_running_counters, inputs, outputs, metrics_to_norms, clear_metrics_op, \n",
    "            update_metrics_op, configuration, device=device, verbose=is_chief) \n",
    "        # Add metrics\n",
    "        graph_manager.add_metrics_to_graph(\n",
    "            eval_utils.get_standard_eval, inputs, outputs, metrics_to_norms, clear_metrics_op, \n",
    "            update_metrics_op, configuration, device=device, verbose=is_chief)     \n",
    "    return outputs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid size [32 32]\n",
      "Retrieval top k = 1024 (final)\n",
      "\n",
      "\u001b[44mLoad inputs:\u001b[0m\n",
      "    with default `num_threads` = 8\n",
      "    with default `prefetch_capacity` = 1\n",
      "    with default `data_augmentation_threshold` = 0.5\n",
      "    with default `with_groups` = False\n",
      "    with default `with_classification` = False\n",
      "    pad \u001b[32mtrain\u001b[0m inputs with \u001b[32m0\u001b[0m dummy samples\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[None, 100, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mnum_boxes\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mimage\u001b[0m: shape=[None, 1024, 1024, 3], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mobj_i_mask_bbs\u001b[0m: shape=[None, 32, 32, 1, 100], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mim_id\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mis_flipped\u001b[0m: shape=[None], dtype=<dtype: 'float32'>\n",
      "\u001b[37minputs(train) graph: 0.00 MB\u001b[0m\n",
      "\n",
      "\u001b[43mTrain Graph:\u001b[0m\n",
      " \u001b[34mtinyyolov2:\u001b[0m\n",
      "  > Use custom \u001b[32mtiny yolo v2\u001b[0m\n",
      "    with default `weight_decay` = 0.0\n",
      "    with default `normalizer_decay` = 0.9\n",
      "    with default `num_filters` = [16, 32, 64, 128, 256, 512, 1024]\n",
      "    with default `with_classification` = False\n",
      "    Output layer shape\u001b[32m (?, 32, 32, 1, 5) \u001b[0m\n",
      "    with default `target_conf_fn` = iou\n",
      "    with default `assignment_reward_fn` = iou\n",
      "    \u001b[32mlog_scales\u001b[0m: shape=[None, 32, 32, 1, 2], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[None, 32, 32, 1, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mconfidence_scores\u001b[0m: shape=[None, 32, 32, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mdetection_scores\u001b[0m: shape=[None, 32, 32, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mtarget_bounding_boxes\u001b[0m: shape=[None, 32, 32, 1, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mshifted_centers\u001b[0m: shape=[None, 32, 32, 1, 2], dtype=<dtype: 'float32'>\n",
      " \u001b[34msummaries:\u001b[0m\n",
      "    with default `num_summaries` = 3\n",
      "    with default `summary_confidence_thresholds` = [0.5]\n",
      "\u001b[37mtrain net (gpu:0) graph: 0.05 MB\u001b[0m\n",
      "\u001b[37mtrain net (gpu:1) graph: 0.08 MB\u001b[0m\n",
      "  > Collect losses\n",
      "\u001b[37mfull loss graph: 0.08 MB\u001b[0m\n",
      "  > Build train operation\n",
      "    with default `optimizer` = ADAM\n",
      "  > Using optimizer \u001b[32mADAM\u001b[0m with learning rate \u001b[32m5.00e-04\u001b[0m\n",
      "    with default `beta1` = 0.9\n",
      "  > 32 update operations found\n",
      "\u001b[37mtrain op graph: 0.17 MB\u001b[0m\n",
      "\n",
      "\u001b[43mLosses:\u001b[0m\n",
      "    \u001b[35mtinyyolov2_confidence_noobj_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_centers_localization_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_scales_localization_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_confidence_obj_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_classification_loss:\u001b[0m 2 tensors\n",
      "\n",
      "\u001b[43mTest Graph:\u001b[0m\n",
      "    pad \u001b[32mtest\u001b[0m inputs with \u001b[32m5\u001b[0m dummy samples\n",
      "\u001b[37minputs(test) graph: 0.17 MB\u001b[0m\n",
      " \u001b[34mtinyyolov2:\u001b[0m\n",
      "  > Use custom \u001b[32mtiny yolo v2\u001b[0m\n",
      "    with default `weight_decay` = 0.0\n",
      "    with default `normalizer_decay` = 0.9\n",
      "    with default `num_filters` = [16, 32, 64, 128, 256, 512, 1024]\n",
      "    with default `with_classification` = False\n",
      "    Output layer shape\u001b[32m (?, 32, 32, 1, 5) \u001b[0m\n",
      "    with default `retrieval_confidence_threshold` = 0.0\n",
      "    with default `retrieval_nms_threshold` = 0.5\n",
      "    with default `num_summaries` = 3\n",
      "    with default `summary_confidence_thresholds` = [0.5]\n",
      "\u001b[37mtest net (gpu:0) graph: 0.20 MB\u001b[0m\n",
      "\u001b[37mtest net (gpu:1) graph: 0.23 MB\u001b[0m\n",
      "    \u001b[32m16\u001b[0m eval update ops\n",
      "    \u001b[32m16\u001b[0m eval clear ops\n",
      "\n",
      "\u001b[43mEval metrics:\u001b[0m\n",
      "    \u001b[35mnum_samples_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_avgprec_at0.75_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_maxrecall_at0.25_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mnum_valid_samples_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_maxrecall_at0.50_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_avgprec_at0.25_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_avgprec_at0.50_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mtinyyolov2_maxrecall_at0.75_eval:\u001b[0m 2 tensors\n",
      "\n",
      "\u001b[44mLaunch session:\u001b[0m\n",
      "    with default `base_log_dir` = ./log\n",
      "    Log directory /home/aroyer/ODGI/log/sdd/yolov2_1024/06-06_10-35\n",
      "    with default `max_to_keep` = 1\n",
      "    with default `log_globalstep_steps` = 100\n",
      "\n",
      "\u001b[44mStart training:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b21fd484636f4ddaa74bfbabc1b85bb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HTML</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################## Config\n",
    "vanilla_configuration = configuration.copy()\n",
    "vanilla_configuration['base_name'] =  'tinyyolov2'\n",
    "\n",
    "# Set resolution parameter [I, J] and K\n",
    "vanilla_configuration['image_size'] = 1024\n",
    "vanilla_configuration['num_boxes'] = 1\n",
    "\n",
    "# Finalize\n",
    "vanilla_configuration['exp_name'] += '/yolov2_%d' % vanilla_configuration['image_size']\n",
    "graph_manager.finalize_grid_offsets(vanilla_configuration)\n",
    "print('Retrieval top k = %d (final)' % vanilla_configuration['retrieval_top_n'])\n",
    "\n",
    "\n",
    "with tf.Graph().as_default() as graph:          \n",
    "    ########################################################################## Train graph\n",
    "    with tf.name_scope('train'):\n",
    "        print('\\n\\033[44mLoad inputs:\\033[0m')\n",
    "        inputs = graph_manager.get_inputs(mode='train', verbose=True, **vanilla_configuration)   \n",
    "        viz.display_graph_size('inputs(train)')  \n",
    "        \n",
    "        print('\\n\\033[43mTrain Graph:\\033[0m')      \n",
    "        for i, train_inputs in enumerate(inputs):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('dev%d' % i):\n",
    "                    is_chief = (i == 0)\n",
    "                    train_outputs = train_pass(train_inputs, vanilla_configuration, is_chief=is_chief)   \n",
    "                    if is_chief:\n",
    "                        print(' \\033[34msummaries:\\033[0m')\n",
    "                        graph_manager.add_summaries(\n",
    "                            train_inputs, train_outputs, mode='train', **vanilla_configuration)\n",
    "            viz.display_graph_size('train net (gpu:%d)' % i)\n",
    "\n",
    "        # Training Objective\n",
    "        with tf.name_scope('losses'):\n",
    "            losses = graph_manager.get_total_loss()\n",
    "            full_loss = tf.add_n([x[0] for x in losses])\n",
    "        viz.display_graph_size('full loss')\n",
    "\n",
    "        # Train op    \n",
    "        with tf.name_scope('train_op'):   \n",
    "            global_step, train_op = graph_manager.get_train_op(losses, **vanilla_configuration)\n",
    "        viz.display_graph_size('train op')\n",
    "        \n",
    "        # Additional info\n",
    "        with tf.name_scope('config_summary'):\n",
    "            viz.add_text_summaries(vanilla_configuration) \n",
    "            print('\\n\\033[43mLosses:\\033[0m')\n",
    "            print('\\n'.join([\"    \\033[35m%s:\\033[0m %s tensors\" % (x, len(tf.get_collection(x)))  \n",
    "                            for x in tf.get_default_graph().get_all_collection_keys() if x.endswith('_loss')]))\n",
    "    \n",
    "    \n",
    "    ##########################################################################  Evaluation graph\n",
    "    with tf.name_scope('eval'):        \n",
    "        print('\\n\\033[43mTest Graph:\\033[0m')\n",
    "        update_metrics_op = []    # Store operations to update the metrics\n",
    "        clear_metrics_op = []     # Store operations to reset the metrics\n",
    "        metrics_to_norms = {}\n",
    "\n",
    "        inputs = graph_manager.get_inputs(mode='test', verbose=False, **vanilla_configuration)         \n",
    "        viz.display_graph_size('inputs(test)')            \n",
    "\n",
    "        for i, val_inputs in enumerate(inputs):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('dev%d' % i):\n",
    "                    is_chief = (i == 0)\n",
    "                    val_outputs = eval_pass(val_inputs, vanilla_configuration, metrics_to_norms, \n",
    "                                            clear_metrics_op, update_metrics_op, device=i, is_chief=is_chief) \n",
    "                    if is_chief:\n",
    "                        graph_manager.add_summaries(\n",
    "                            val_inputs, val_outputs, mode='test', **vanilla_configuration)   \n",
    "            viz.display_graph_size('test net (gpu:%d)' % i)\n",
    "\n",
    "        with tf.name_scope('eval'):\n",
    "            print('    \\x1b[32m%d\\x1b[0m eval update ops' % len(update_metrics_op))\n",
    "            print('    \\x1b[32m%d\\x1b[0m eval clear ops' % len(clear_metrics_op))\n",
    "            update_metrics_op = tf.group(*update_metrics_op)\n",
    "            clear_metrics_op = tf.group(*clear_metrics_op)\n",
    "            eval_summary_op = graph_manager.get_eval_op(metrics_to_norms)\n",
    "\n",
    "        # Additional info\n",
    "        print('\\n\\033[43mEval metrics:\\033[0m')\n",
    "        print('\\n'.join([\"    \\033[35m%s:\\033[0m %s tensors\" % (x, len(tf.get_collection(x)))  \n",
    "                        for x in tf.get_default_graph().get_all_collection_keys() \n",
    "                        if x.endswith('_eval')]))\n",
    "\n",
    "    ########################################################################## Run    \n",
    "    try:\n",
    "        print('\\n\\033[44mLaunch session:\\033[0m')\n",
    "        graph_manager.generate_log_dir(vanilla_configuration)\n",
    "        summary_writer = SummaryWriterCache.get(vanilla_configuration[\"log_dir\"])\n",
    "        print('    Log directory', os.path.abspath(vanilla_configuration[\"log_dir\"]))\n",
    "        \n",
    "        with graph_manager.get_monitored_training_session(**vanilla_configuration) as sess:    \n",
    "            loss_widget = widgets.HTML(value=\"\")\n",
    "            start_time = time.time()\n",
    "            global_step_ = 0            \n",
    "            print('\\n\\033[44mStart training:\\033[0m')\n",
    "            display(loss_widget)   \n",
    "            while not sess.should_stop(): \n",
    "                        \n",
    "                # Train\n",
    "                global_step_, full_loss_, _ = sess.run([global_step, full_loss, train_op])\n",
    "                \n",
    "                # Evaluate\n",
    "                if (vanilla_configuration[\"save_evaluation_steps\"] is not None and (global_step_ > 1)\n",
    "                    and global_step_  % vanilla_configuration[\"save_evaluation_steps\"] == 0):\n",
    "                    sess.run(clear_metrics_op)\n",
    "                    num_epochs = vanilla_configuration[\"test_num_iters_per_epoch\"]\n",
    "                    for epoch in range(num_epochs):\n",
    "                        viz.display_eval(loss_widget, global_step_, epoch + 1, num_epochs, start_time)\n",
    "                        sess.run(update_metrics_op) \n",
    "                        if epoch == num_epochs - 1: \n",
    "                            eval_summary = sess.run(eval_summary_op)\n",
    "\n",
    "                    # Write summary\n",
    "                    summary_writer.add_summary(eval_summary, global_step_)\n",
    "                    summary_writer.flush()\n",
    "                    \n",
    "                # Display\n",
    "                if (global_step_ - 1) % 20 == 0:\n",
    "                    viz.display_loss(loss_widget, global_step_, full_loss_, start_time, \n",
    "                                     vanilla_configuration[\"train_num_samples_per_iter\"], \n",
    "                                     vanilla_configuration[\"train_num_samples\"])\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nInterrupted at step %d' % global_step_)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "bea7b664d55547eeb27eef7df2421f86": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
