{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0, 1\"\n",
    "from math import ceil\n",
    "\n",
    "import time\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.training.summary_io import SummaryWriterCache\n",
    "\n",
    "import graph_manager\n",
    "import net\n",
    "import eval_utils\n",
    "import loss_utils\n",
    "import tf_inputs\n",
    "import tf_utils\n",
    "import viz\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Configuration\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39688 training steps\n",
      "...which means 121 epochs\n",
      "10476 training samples (328 iters)\n",
      "2619 validation samples (82 iters)\n",
      "\n",
      "\u001b[41mConfig:\u001b[0m\n",
      "\u001b[96mbatch_size:\u001b[0m 16\n",
      "\u001b[96mcenters_localization_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mconfidence_loss_weight:\u001b[0m 5.0\n",
      "\u001b[96mdata_classes:\u001b[0m ['Biker', 'Bus', 'Car', 'Cart', 'Pedestrian', 'Skater']\n",
      "\u001b[96mexp_name:\u001b[0m sdd\n",
      "\u001b[96mfeature_keys:\u001b[0m ['im_id', 'num_boxes', 'bounding_boxes', 'classes']\n",
      "\u001b[96mgpu_mem_frac:\u001b[0m 1.0\n",
      "\u001b[96mgroup_classification_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mimage_folder:\u001b[0m /home/aroyer/Datasets/sdd_images\n",
      "\u001b[96mlast_test_batch_size:\u001b[0m 27\n",
      "\u001b[96mlearning_rate:\u001b[0m 0.001\n",
      "\u001b[96mnoobj_confidence_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mnum_classes:\u001b[0m 6\n",
      "\u001b[96mnum_epochs:\u001b[0m 120\n",
      "\u001b[96mnum_gpus:\u001b[0m 2\n",
      "\u001b[96mnum_steps:\u001b[0m 39688\n",
      "\u001b[96moffsets_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96mretrieval_intersection_threshold:\u001b[0m [0.25, 0.5, 0.75]\n",
      "\u001b[96msave_checkpoint_secs:\u001b[0m 3600\n",
      "\u001b[96msave_evaluation_steps:\u001b[0m 500\n",
      "\u001b[96msave_summaries_steps:\u001b[0m 200\n",
      "\u001b[96mscales_localization_loss_weight:\u001b[0m 1.0\n",
      "\u001b[96msetting:\u001b[0m sdd\n",
      "\u001b[96mshuffle_buffer:\u001b[0m 2000\n",
      "\u001b[96msubset:\u001b[0m -1\n",
      "\u001b[96mtest_batch_size:\u001b[0m 16\n",
      "\u001b[96mtest_max_num_bbs:\u001b[0m 100\n",
      "\u001b[96mtest_num_iters_per_epoch:\u001b[0m 82\n",
      "\u001b[96mtest_num_samples:\u001b[0m 2619\n",
      "\u001b[96mtest_num_samples_per_iter:\u001b[0m 32\n",
      "\u001b[96mtest_tfrecords:\u001b[0m Data/sdd_test\n",
      "\u001b[96mtrain_max_num_bbs:\u001b[0m 100\n",
      "\u001b[96mtrain_num_iters_per_epoch:\u001b[0m 328\n",
      "\u001b[96mtrain_num_samples:\u001b[0m 10476\n",
      "\u001b[96mtrain_num_samples_per_iter:\u001b[0m 32\n",
      "\u001b[96mtrain_tfrecords:\u001b[0m Data/sdd_train\n"
     ]
    }
   ],
   "source": [
    "data = 'stanford'\n",
    "\n",
    "configuration = {}\n",
    "if data == 'vedai':\n",
    "    configuration['setting'] = 'vedai'\n",
    "    configuration['exp_name'] = 'vedai'\n",
    "    configuration['save_summaries_steps'] = 100\n",
    "    configuration['save_evaluation_steps'] = 250\n",
    "    configuration['num_epochs'] = 1000\n",
    "elif data == 'stanford':\n",
    "    configuration['setting'] = 'sdd'\n",
    "    configuration['exp_name'] = 'sdd'\n",
    "    configuration['save_summaries_steps'] = 200\n",
    "    configuration['save_evaluation_steps'] = 500\n",
    "    configuration['num_epochs'] = 120\n",
    "    \n",
    "## Metadata\n",
    "tfrecords_path = '/home/aroyer/indolentDetect/Data/metadata_%s.txt'\n",
    "metadata = graph_manager.load_metadata(tfrecords_path % configuration['setting'])\n",
    "configuration.update(metadata)\n",
    "configuration['num_classes'] = len(configuration['data_classes'])\n",
    "\n",
    "## GPUs\n",
    "configuration['num_gpus'] = 2                                 \n",
    "configuration['gpu_mem_frac'] = 1.\n",
    "\n",
    "## Inputs Pipeline\n",
    "configuration['subset'] = -1\n",
    "configuration['batch_size'] = 16\n",
    "configuration['test_batch_size'] = 16\n",
    "configuration['shuffle_buffer'] = 2000\n",
    "    \n",
    "## Evaluation\n",
    "configuration['save_checkpoint_secs'] = 3600\n",
    "configuration['retrieval_intersection_threshold'] = [0.25, 0.5, 0.75]\n",
    "\n",
    "## Training\n",
    "configuration['learning_rate'] = 1e-3\n",
    "configuration['centers_localization_loss_weight'] = 1.\n",
    "configuration['scales_localization_loss_weight']  = 1.\n",
    "configuration['confidence_loss_weight']  = 5.\n",
    "configuration['noobj_confidence_loss_weight']  = 1.\n",
    "configuration['group_classification_loss_weight']  = 1.\n",
    "configuration['offsets_loss_weight']  = 1.\n",
    "\n",
    "graph_manager.finalize_configuration(configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(inputs, \n",
    "                 outputs, \n",
    "                 configuration,\n",
    "                 is_training=True,\n",
    "                 reuse=False, \n",
    "                 verbose=False,\n",
    "                 scope_name='model'):\n",
    "    \"\"\"Forward-pass in the net\"\"\"\n",
    "    with tf.variable_scope(scope_name, reuse=reuse):        \n",
    "        activations = net.tiny_yolo_v2(\n",
    "            inputs[\"image\"], is_training=is_training, reuse=reuse, verbose=verbose, **configuration)\n",
    "        net.get_detection_with_groups_outputs(\n",
    "            activations, outputs, reuse=reuse, verbose=verbose, **configuration)\n",
    "            \n",
    "            \n",
    "def train_pass(inputs, configuration, intermediate_stage=False, is_chief=False):\n",
    "    \"\"\" Compute outputs of the net and add losses to the graph.\n",
    "    \"\"\"\n",
    "    outputs = {}\n",
    "    base_name = graph_manager.get_defaults(configuration, ['base_name'], verbose=is_chief)[0]\n",
    "    if is_chief: print(' \\033[34m%s:\\033[0m' % base_name)\n",
    "        \n",
    "    # Feed forward\n",
    "    with tf.name_scope('%s/net' % base_name):\n",
    "        forward_pass(inputs, outputs, configuration, scope_name=base_name, \n",
    "                     is_training=True, reuse=not is_chief, verbose=is_chief) \n",
    "        \n",
    "    # Compute crops to feed to the next stage\n",
    "    if intermediate_stage:\n",
    "        with tf.name_scope('extract_patches'):\n",
    "            tf_inputs.extract_groups(inputs, outputs, mode='train', verbose=is_chief, **configuration)  \n",
    "        \n",
    "    # Add losses\n",
    "    with tf.name_scope('%s/loss' % base_name):\n",
    "        if intermediate_stage:\n",
    "            loss_fn = loss_utils.get_odgi_loss\n",
    "        else:\n",
    "            loss_fn = loss_utils.get_standard_loss\n",
    "        graph_manager.add_losses_to_graph(\n",
    "            loss_fn, inputs, outputs, configuration, is_chief=is_chief, verbose=is_chief)\n",
    "        \n",
    "    if is_chief:\n",
    "        print('\\n'.join(\"    \\033[32m%s\\033[0m: shape=%s, dtype=%s\" % (\n",
    "            key, value.get_shape().as_list(), value.dtype) for key, value in outputs.items()))\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def feed_pass(inputs, outputs, configuration, mode='train', is_chief=False):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            inputs: inputs dictionnary\n",
    "            outputs: outputs dictionnary\n",
    "            configuration: config dictionnary\n",
    "        \n",
    "        Returns:\n",
    "            Dictionnary of inputs for the next stage\n",
    "    \"\"\"\n",
    "    if is_chief: print(' \\033[34mextract patches:\\033[0m')\n",
    "    return graph_manager.get_stage2_inputs(\n",
    "        inputs, outputs['crop_boxes'], mode=mode, verbose=is_chief, **configuration)\n",
    "        \n",
    "    \n",
    "def eval_pass_intermediate_stage(inputs, configuration, metrics_to_norms, clear_metrics_op, \n",
    "                                 update_metrics_op, device=0, is_chief=False):\n",
    "    \"\"\" Evaluation pass for intermediate stages.\"\"\"\n",
    "    outputs = {}\n",
    "    base_name = graph_manager.get_defaults(configuration, ['base_name'], verbose=is_chief)[0]\n",
    "    if is_chief: print(' \\033[34m%s:\\033[0m' % base_name)\n",
    "        \n",
    "    # Feed forward\n",
    "    with tf.name_scope('%s/net' % base_name):\n",
    "        forward_pass(inputs, outputs, configuration, scope_name=base_name, is_training=False, \n",
    "                     reuse=True, verbose=is_chief) \n",
    "        \n",
    "    # Compute crops to feed to the next stage\n",
    "    with tf.name_scope('extract_patches'):\n",
    "        tf_inputs.extract_groups(inputs, outputs, mode='test', verbose=is_chief, **configuration)        \n",
    "        \n",
    "    with tf.name_scope('%s/eval' % base_name):\n",
    "        # Add number of samples counter\n",
    "        graph_manager.add_metrics_to_graph(\n",
    "            eval_utils.get_samples_running_counters, inputs, outputs, metrics_to_norms, clear_metrics_op, \n",
    "            update_metrics_op, configuration, device=device, verbose=is_chief) \n",
    "        # Add metrics\n",
    "        graph_manager.add_metrics_to_graph(\n",
    "            eval_utils.get_odgi_eval, inputs, outputs, metrics_to_norms, clear_metrics_op, \n",
    "            update_metrics_op, configuration, device=device, verbose=is_chief)     \n",
    "        \n",
    "    return outputs    \n",
    "\n",
    "\n",
    "def eval_pass_final_stage(stage2_inputs, stage1_inputs, stage1_outputs, configuration, metrics_to_norms, \n",
    "                          clear_metrics_op, update_metrics_op, device=0, is_chief=False):\n",
    "    \"\"\" Evaluation for the full pipeline.\n",
    "        Args:\n",
    "            stage2_inputs: inputs dictionnary for stage2\n",
    "            stage1_inputs: inputs dictionnary for stage 1\n",
    "            stage1_outputs: outputs dictionnary for stage1\n",
    "            configuration: config dictionnary\n",
    "            metrics_to_norms: Map metrics key to normalizer key\n",
    "            clear_metrics_op: List to be updated with reset operations\n",
    "            update_metrics_op: List to be updated with update operation\n",
    "            device: Current device number to be used in the variable scope for each metric\n",
    "        \n",
    "        Returns:\n",
    "            Dictionnary of outputs, merge by image\n",
    "            Dictionnary of unscaled ouputs (for summary purposes)\n",
    "    \"\"\"\n",
    "    base_name = graph_manager.get_defaults(configuration, ['base_name'], verbose=is_chief)[0]\n",
    "    if is_chief: print(' \\033[34m%s:\\033[0m' % base_name)\n",
    "    outputs = {}\n",
    "    \n",
    "    # Feed forward\n",
    "    with tf.name_scope('net'):\n",
    "        forward_pass(stage2_inputs, outputs, configuration, scope_name=base_name,\n",
    "                     is_training=False, reuse=True, verbose=is_chief) \n",
    "            \n",
    "    # Reshape outputs from stage2 to stage1\n",
    "    # for summary\n",
    "    unscaled_outputs = {key: outputs[key] for key in ['bounding_boxes', 'detection_scores']} \n",
    "    with tf.name_scope('reshape_outputs'):\n",
    "        crop_boxes = stage1_outputs[\"crop_boxes\"]  \n",
    "        num_crops = crop_boxes.get_shape()[1].value\n",
    "        num_boxes = outputs['bounding_boxes'].get_shape()[-2].value\n",
    "        batch_size = graph_manager.get_defaults(configuration, ['test_batch_size'], verbose=is_chief)[0]\n",
    "        # outputs:  (stage1_batch * num_crops, num_cell, num_cell, num_boxes, ...)\n",
    "        # to: (stage1_batch, num_cell, num_cell, num_boxes * num_crops, ...)\n",
    "        for key, value in outputs.items():\n",
    "            shape = tf.shape(value)\n",
    "            batches = tf.split(value, batch_size, axis=0)\n",
    "            batches = [tf.concat(tf.unstack(b, num=num_crops, axis=0), axis=2) for b in batches]\n",
    "            outputs[key] = tf.stack(batches, axis=0)\n",
    "    \n",
    "    # Rescale bounding boxes from stage2 to stage1\n",
    "    with tf.name_scope('rescale_bounding_boxes'):\n",
    "        # crop_boxes: (stage1_batch, 1, 1, num_crops * num_boxes, 4)\n",
    "        crop_boxes = tf.reshape(crop_boxes, (batch_size, num_crops, 1, 4))\n",
    "        crop_boxes = tf.tile(crop_boxes, (1, 1, num_boxes, 1))\n",
    "        crop_boxes = tf.reshape(crop_boxes, (batch_size, 1, 1, num_crops * num_boxes, 4))\n",
    "        crop_mins, crop_maxs = tf.split(crop_boxes, 2, axis=-1)\n",
    "        # bounding_boxes: (stage1_batch, num_cells, num_cells, num_crops * num_boxes, 4)\n",
    "        bounding_boxes = outputs['bounding_boxes']\n",
    "        bounding_boxes *= tf.maximum(1e-8, tf.tile(crop_maxs - crop_mins, (1, 1, 1, 1, 2)))\n",
    "        bounding_boxes += tf.tile(crop_mins, (1, 1, 1, 1, 2))\n",
    "        bounding_boxes = tf.clip_by_value(bounding_boxes, 0., 1.)\n",
    "        outputs['bounding_boxes'] = bounding_boxes\n",
    "            \n",
    "    ## Add the additional bounding boxes outputs propagated from earlier stages\n",
    "    if 'added_bounding_boxes' in stage1_outputs:\n",
    "        assert 'added_detection_scores' in stage1_outputs\n",
    "        outputs['bounding_boxes'] = tf_utils.flatten_percell_output(outputs['bounding_boxes'])\n",
    "        outputs['bounding_boxes'] = tf.concat([outputs['bounding_boxes'],\n",
    "                                               stage1_outputs['added_bounding_boxes']], axis=1)\n",
    "        outputs['detection_scores'] = tf_utils.flatten_percell_output(outputs['detection_scores'])\n",
    "        outputs['detection_scores'] = tf.concat([outputs['detection_scores'],\n",
    "                                                stage1_outputs['added_detection_scores']], axis=1)\n",
    "        \n",
    "    # Evaluate `output` versus the initial (stage 1) inputs\n",
    "    with tf.name_scope('eval'):\n",
    "        graph_manager.add_metrics_to_graph(\n",
    "            eval_utils.get_standard_eval, stage1_inputs, outputs, metrics_to_norms, clear_metrics_op, \n",
    "            update_metrics_op, configuration, device=device, verbose=is_chief)\n",
    "    return outputs, unscaled_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multistage YOLO\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid size [16 16]\n",
      "grid size [8 8]\n",
      "Retrieval top k = 576 (final)\n",
      "\n",
      "\u001b[44mLoad inputs:\u001b[0m\n",
      "    with default `num_threads` = 8\n",
      "    with default `prefetch_capacity` = 1\n",
      "    with default `data_augmentation_threshold` = 0.5\n",
      "    with default `with_classification` = False\n",
      "    pad \u001b[32mtrain\u001b[0m inputs with \u001b[32m0\u001b[0m dummy samples\n",
      "    \u001b[32mimage\u001b[0m: shape=[None, 512, 512, 3], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mgroup_bounding_boxes_per_cell\u001b[0m: shape=[None, 16, 16, 1, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mnum_boxes\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[None, 100, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mnum_group_boxes\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mim_id\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mobj_i_mask_bbs\u001b[0m: shape=[None, 16, 16, 1, 100], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mis_flipped\u001b[0m: shape=[None], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mgroup_flags\u001b[0m: shape=[None, 16, 16, 1, 1], dtype=<dtype: 'float32'>\n",
      "\n",
      "\u001b[43mTrain Graph:\u001b[0m\n",
      "\u001b[37minputs(train) graph: 0.00 MB\u001b[0m\n",
      " \u001b[34mstage1:\u001b[0m\n",
      "  > Use custom \u001b[32mtiny yolo v2\u001b[0m\n",
      "    with default `weight_decay` = 0.0\n",
      "    with default `normalizer_decay` = 0.9\n",
      "    with default `num_filters` = [16, 32, 64, 128, 256, 512, 1024]\n",
      "    with default `with_classification` = False\n",
      "    Output layer shape\u001b[32m (?, 16, 16, 1, 8) \u001b[0m\n",
      "    with default `train_patch_confidence_threshold` = 0.15\n",
      "    with default `patch_nms_threshold` = 0.25\n",
      "    with default `train_num_crops` = 5\n",
      "    extracting \u001b[32m5\u001b[0m crops\n",
      "    with default `target_conf_fn` = iou\n",
      "    with default `offsets_margin` = 0.025\n",
      "    \u001b[32moffsets\u001b[0m: shape=[None, 16, 16, 1, 2], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mcrop_boxes\u001b[0m: shape=[None, 5, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mtarget_bounding_boxes\u001b[0m: shape=[None, 16, 16, 1, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mlog_scales\u001b[0m: shape=[None, 16, 16, 1, 2], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[None, 16, 16, 1, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mdetection_scores\u001b[0m: shape=[None, 16, 16, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mshifted_centers\u001b[0m: shape=[None, 16, 16, 1, 2], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mconfidence_scores\u001b[0m: shape=[None, 16, 16, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mgroup_classification_logits\u001b[0m: shape=[None, 16, 16, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mcrop_boxes_confidences\u001b[0m: shape=[None, 5], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mtarget_bounding_boxes_rescaled\u001b[0m: shape=[None, 16, 16, 1, 4], dtype=<dtype: 'float32'>\n",
      " \u001b[34mextract patches:\u001b[0m\n",
      "    with default `patch_intersection_ratio_threshold` = 0.3\n",
      "    with default `num_threads` = 8\n",
      "    \u001b[32mimage\u001b[0m: shape=[16, 256, 256, 3], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mnum_boxes\u001b[0m: shape=[16], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[16, 100, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mim_id\u001b[0m: shape=[16], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mobj_i_mask_bbs\u001b[0m: shape=[16, 8, 8, 1, 100], dtype=<dtype: 'float32'>\n",
      " \u001b[34mstage2:\u001b[0m\n",
      "  > Use custom \u001b[32mtiny yolo v2\u001b[0m\n",
      "    with default `weight_decay` = 0.0\n",
      "    with default `normalizer_decay` = 0.9\n",
      "    with default `num_filters` = [16, 32, 64, 128, 256, 512, 1024]\n",
      "    with default `with_classification` = False\n",
      "    with default `with_group_flags` = False\n",
      "    with default `with_offsets` = False\n",
      "    Output layer shape\u001b[32m (16, 8, 8, 1, 5) \u001b[0m\n",
      "    with default `target_conf_fn` = iou\n",
      "    with default `assignment_reward_fn` = iou\n",
      "    \u001b[32mlog_scales\u001b[0m: shape=[16, 8, 8, 1, 2], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[16, 8, 8, 1, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mdetection_scores\u001b[0m: shape=[16, 8, 8, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mshifted_centers\u001b[0m: shape=[16, 8, 8, 1, 2], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mconfidence_scores\u001b[0m: shape=[16, 8, 8, 1, 1], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mtarget_bounding_boxes\u001b[0m: shape=[16, 8, 8, 1, 4], dtype=<dtype: 'float32'>\n",
      " \u001b[34msummaries:\u001b[0m\n",
      "    with default `num_summaries` = 3\n",
      "    with default `summary_confidence_thresholds` = [0.5]\n",
      "\u001b[37mtrain net (gpu:0) graph: 0.10 MB\u001b[0m\n",
      "\u001b[37mtrain net (gpu:1) graph: 0.19 MB\u001b[0m\n",
      "  > Collect losses (scopes: stage1,stage2)\n",
      "\u001b[37mfull loss graph: 0.19 MB\u001b[0m\n",
      "  > Build train operation\n",
      "    with default `optimizer` = ADAM\n",
      "  > Using optimizer \u001b[32mADAM\u001b[0m with learning rate \u001b[32m1.00e-03\u001b[0m\n",
      "    with default `beta1` = 0.9\n",
      "  > 64 update operations found\n",
      "\u001b[37mtrain op graph: 0.36 MB\u001b[0m\n",
      "\n",
      "\u001b[43mLosses:\u001b[0m\n",
      "    \u001b[35mstage2_classification_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_centers_localization_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_scales_localization_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_confidence_noobj_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_centers_localization_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_confidence_obj_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_confidence_noobj_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_group_classification_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_scales_localization_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_confidence_obj_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_offsets_loss:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_classification_loss:\u001b[0m 2 tensors\n",
      "\n",
      "\u001b[43mVal Graph:\u001b[0m\n",
      "    pad \u001b[32mtest\u001b[0m inputs with \u001b[32m5\u001b[0m dummy samples\n",
      "\u001b[37minputs(test) graph: 0.37 MB\u001b[0m\n",
      " \u001b[34mstage1:\u001b[0m\n",
      "  > Use custom \u001b[32mtiny yolo v2\u001b[0m\n",
      "    with default `weight_decay` = 0.0\n",
      "    with default `normalizer_decay` = 0.9\n",
      "    with default `num_filters` = [16, 32, 64, 128, 256, 512, 1024]\n",
      "    with default `with_classification` = False\n",
      "    Output layer shape\u001b[32m (?, 16, 16, 1, 8) \u001b[0m\n",
      "    with default `test_patch_confidence_threshold` = 0.25\n",
      "    with default `patch_nms_threshold` = 0.25\n",
      "    with default `test_num_crops` = 5\n",
      "    extracting \u001b[32m5\u001b[0m crops\n",
      "    with default `test_patch_strong_confidence_threshold` = 0.75\n",
      "    with default `retrieval_confidence_threshold` = 0.0\n",
      "    with default `retrieval_nms_threshold` = 0.5\n",
      " \u001b[34mextract patches:\u001b[0m\n",
      "    with default `patch_intersection_ratio_threshold` = 0.3\n",
      "    with default `test_num_crops` = 5\n",
      "    \u001b[32mimage\u001b[0m: shape=[None, 256, 256, 3], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mnum_boxes\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mbounding_boxes\u001b[0m: shape=[None, 100, 4], dtype=<dtype: 'float32'>\n",
      "    \u001b[32mim_id\u001b[0m: shape=[None], dtype=<dtype: 'int32'>\n",
      "    \u001b[32mobj_i_mask_bbs\u001b[0m: shape=[None, 8, 8, 1, 100], dtype=<dtype: 'float32'>\n",
      " \u001b[34mstage2:\u001b[0m\n",
      "  > Use custom \u001b[32mtiny yolo v2\u001b[0m\n",
      "    with default `weight_decay` = 0.0\n",
      "    with default `normalizer_decay` = 0.9\n",
      "    with default `num_filters` = [16, 32, 64, 128, 256, 512, 1024]\n",
      "    with default `with_classification` = False\n",
      "    with default `with_group_flags` = False\n",
      "    with default `with_offsets` = False\n",
      "    Output layer shape\u001b[32m (?, 8, 8, 1, 5) \u001b[0m\n",
      "    with default `retrieval_confidence_threshold` = 0.0\n",
      "    with default `retrieval_nms_threshold` = 0.5\n",
      "    with default `num_summaries` = 3\n",
      "    with default `summary_confidence_thresholds` = [0.5]\n",
      "\u001b[37mtest net (gpu:0) graph: 0.45 MB\u001b[0m\n",
      "\u001b[37mtest net (gpu:1) graph: 0.53 MB\u001b[0m\n",
      "    \u001b[32m32\u001b[0m eval update ops\n",
      "    \u001b[32m32\u001b[0m eval clear ops\n",
      "\n",
      "\u001b[43mEval metrics:\u001b[0m\n",
      "    \u001b[35mstage1_maxrecall_at0.50_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_maxrecall_at0.25_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_avgprec_at0.50_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_avgprec_at0.50_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_avgprec_at0.75_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_maxrecall_at0.75_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mnum_valid_samples_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_maxrecall_at0.50_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_maxrecall_at0.75_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_maxrecall_at0.25_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mnum_samples_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_avgprec_at0.25_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage1_avgprec_at0.25_eval:\u001b[0m 2 tensors\n",
      "    \u001b[35mstage2_avgprec_at0.75_eval:\u001b[0m 2 tensors\n",
      "\n",
      "\u001b[44mLaunch session:\u001b[0m\n",
      "    with default `base_log_dir` = ./log\n",
      "    Log directory /home/aroyer/ODGI/log/sdd/odgi_512_256/06-06_10-35\n",
      "    with default `max_to_keep` = 1\n",
      "    with default `log_globalstep_steps` = 100\n",
      "\n",
      "\u001b[44mStart training:\u001b[0m\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c512ad2404f24e8f93916540adca25d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HTML</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HTML(value='')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "########################################################################## Config\n",
    "multistage_configuration = configuration.copy()\n",
    "multistage_configuration['full_image_size'] = 1024\n",
    "multistage_configuration['num_boxes'] = 1\n",
    "stage1_configuration = multistage_configuration.copy()\n",
    "stage2_configuration = multistage_configuration.copy()\n",
    "\n",
    "# Inputs sizes\n",
    "stage1_configuration['image_size'] = 512\n",
    "stage2_configuration['image_size'] = stage1_configuration['image_size'] // 2\n",
    "\n",
    "# Finalize\n",
    "# stage 1\n",
    "stage1_configuration['num_boxes'] = 1\n",
    "stage1_configuration['base_name'] = 'stage1'\n",
    "stage1_configuration['with_groups'] = True\n",
    "stage1_configuration['with_group_flags'] = True\n",
    "stage1_configuration['with_offsets'] = True\n",
    "graph_manager.finalize_grid_offsets(stage1_configuration)\n",
    "\n",
    "# stage 2\n",
    "stage2_configuration['num_boxes'] = 1\n",
    "stage2_configuration['base_name'] = 'stage2'\n",
    "graph_manager.finalize_grid_offsets(stage2_configuration, finalize_retrieval_top_n=False)\n",
    "multistage_configuration['exp_name'] += '/odgi_%d_%d' % (stage1_configuration['image_size'], \n",
    "                                                         stage2_configuration['image_size'])\n",
    "\n",
    "# Compute the final number of outputs (need to define k in topk for final evaluation)\n",
    "num_outputs_stage1 = (stage1_configuration['num_boxes'] *  stage1_configuration['num_cells'][0] * \n",
    "                      stage1_configuration['num_cells'][1])\n",
    "num_outputs_stage2 = (stage2_configuration['num_boxes'] * stage2_configuration['num_cells'][0] * \n",
    "                      stage2_configuration['num_cells'][1])\n",
    "num_crops, retrieval_top_n = graph_manager.get_defaults(\n",
    "    stage2_configuration, ['test_num_crops', 'retrieval_top_n'], verbose=False)\n",
    "num_outputs_final = num_crops * num_outputs_stage2 + num_outputs_stage1\n",
    "stage2_configuration['retrieval_top_n'] = min(retrieval_top_n, num_outputs_final)\n",
    "print('Retrieval top k = %d (final)' % stage2_configuration['retrieval_top_n'])\n",
    "    \n",
    "\n",
    "with tf.Graph().as_default() as graph:          \n",
    "    ########################################################################## Train graph\n",
    "    with tf.name_scope('train'):\n",
    "        print('\\n\\033[44mLoad inputs:\\033[0m')\n",
    "        inputs = graph_manager.get_inputs(mode='train', verbose=True, **stage1_configuration)   \n",
    "        \n",
    "        print('\\n\\033[43mTrain Graph:\\033[0m')\n",
    "        viz.display_graph_size('inputs(train)')        \n",
    "        for i, train_inputs in enumerate(inputs):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('dev%d' % i):\n",
    "                    is_chief = (i == 0)\n",
    "                    train_s1_outputs = train_pass(train_inputs, stage1_configuration, \n",
    "                                                  intermediate_stage=True, is_chief=is_chief)    \n",
    "                    train_s2_inputs = feed_pass(train_inputs, train_s1_outputs, stage2_configuration,\n",
    "                                                mode='train', is_chief=is_chief)\n",
    "                    train_s2_outputs = train_pass(train_s2_inputs, stage2_configuration,\n",
    "                                                  intermediate_stage=False, is_chief=is_chief) \n",
    "                    if is_chief:\n",
    "                        print(' \\033[34msummaries:\\033[0m')\n",
    "                        graph_manager.add_summaries(train_inputs, train_s1_outputs, mode='train', \n",
    "                                                    family=\"train_stage1\", **stage1_configuration)\n",
    "                        graph_manager.add_summaries(train_s2_inputs, train_s2_outputs, mode='train', verbose=0,\n",
    "                                                    family=\"train_stage2\", **stage2_configuration)\n",
    "            viz.display_graph_size('train net (gpu:%d)' % i)\n",
    "\n",
    "        # Training Objective\n",
    "        with tf.name_scope('losses'):\n",
    "            losses = graph_manager.get_total_loss(splits=['stage1', 'stage2'])            \n",
    "            full_loss = tf.add_n([x[0] for x in losses])\n",
    "        viz.display_graph_size('full loss')\n",
    "\n",
    "        # Train op    \n",
    "        with tf.name_scope('train_op'):   \n",
    "            global_step, train_op = graph_manager.get_train_op(losses, **multistage_configuration)\n",
    "        viz.display_graph_size('train op')\n",
    "        \n",
    "        # Additional info\n",
    "        with tf.name_scope('config_summary'):\n",
    "            viz.add_text_summaries(stage1_configuration, family=\"stage1\") \n",
    "            viz.add_text_summaries(stage2_configuration, family=\"stage2\") \n",
    "            print('\\n\\033[43mLosses:\\033[0m')\n",
    "            print('\\n'.join([\"    \\033[35m%s:\\033[0m %s tensors\" % (x, len(tf.get_collection(x)))  \n",
    "                            for x in tf.get_default_graph().get_all_collection_keys() \n",
    "                            if x.endswith('_loss')]))\n",
    "            \n",
    "    ##########################################################################  Evaluation graph\n",
    "    with tf.name_scope('eval'):        \n",
    "        print('\\n\\033[43mVal Graph:\\033[0m')\n",
    "        update_metrics_op = []    # Store operations to update the metrics\n",
    "        clear_metrics_op = []     # Store operations to reset the metrics\n",
    "        metrics_to_norms = {}\n",
    "        inputs = graph_manager.get_inputs(mode='test', verbose=False, **stage1_configuration)         \n",
    "        viz.display_graph_size('inputs(test)')            \n",
    "        \n",
    "        for i, val_inputs in enumerate(inputs):\n",
    "            with tf.device('/gpu:%d' % i):\n",
    "                with tf.name_scope('dev%d' % i):\n",
    "                    is_chief = (i == 0)\n",
    "                    val_s1_outputs = eval_pass_intermediate_stage(\n",
    "                        val_inputs, stage1_configuration, metrics_to_norms, clear_metrics_op,\n",
    "                        update_metrics_op, device=i, is_chief=is_chief) \n",
    "                    val_s2_inputs = feed_pass(val_inputs, val_s1_outputs, stage2_configuration,\n",
    "                                              mode='test', is_chief=is_chief)\n",
    "                    val_s2_outputs, val_s2_unscaled_outputs = eval_pass_final_stage(\n",
    "                        val_s2_inputs, val_inputs,  val_s1_outputs, stage2_configuration, metrics_to_norms, \n",
    "                        clear_metrics_op, update_metrics_op, device=i, is_chief=is_chief)\n",
    "                    \n",
    "                    if is_chief:\n",
    "                        with tf.name_scope('stage1'):\n",
    "                            graph_manager.add_summaries(val_inputs, val_s1_outputs, mode='test', \n",
    "                                                        **stage1_configuration)   \n",
    "                        with tf.name_scope('stage2'):\n",
    "                            graph_manager.add_summaries(val_s2_inputs, val_s2_unscaled_outputs, mode='test',\n",
    "                                                        verbose=False, **stage2_configuration) \n",
    "                        with tf.name_scope('total_pipeline'):\n",
    "                            graph_manager.add_summaries(val_inputs, val_s2_outputs, mode='test', verbose=False,\n",
    "                                                        display_inputs=False, **stage2_configuration)\n",
    "            viz.display_graph_size('test net (gpu:%d)' % i)\n",
    "\n",
    "        with tf.name_scope('eval'):\n",
    "            print('    \\x1b[32m%d\\x1b[0m eval update ops' % len(update_metrics_op))\n",
    "            print('    \\x1b[32m%d\\x1b[0m eval clear ops' % len(clear_metrics_op))\n",
    "            update_metrics_op = tf.group(*update_metrics_op)\n",
    "            clear_metrics_op = tf.group(*clear_metrics_op)\n",
    "            eval_summary_op = graph_manager.get_eval_op(metrics_to_norms)\n",
    "        \n",
    "        # Additional info\n",
    "        print('\\n\\033[43mEval metrics:\\033[0m')\n",
    "        print('\\n'.join([\"    \\033[35m%s:\\033[0m %s tensors\" % (x, len(tf.get_collection(x)))  \n",
    "                        for x in tf.get_default_graph().get_all_collection_keys() \n",
    "                        if x.endswith('_eval')]))\n",
    "        \n",
    "    ########################################################################## Run    \n",
    "    try:\n",
    "        print('\\n\\033[44mLaunch session:\\033[0m')\n",
    "        graph_manager.generate_log_dir(multistage_configuration)\n",
    "        summary_writer = SummaryWriterCache.get(multistage_configuration[\"log_dir\"])\n",
    "        print('    Log directory', os.path.abspath(multistage_configuration[\"log_dir\"]))\n",
    "        \n",
    "        with graph_manager.get_monitored_training_session(**multistage_configuration) as sess:    \n",
    "            loss_widget = widgets.HTML(value=\"\")\n",
    "            global_step_ = 0\n",
    "            start_time = time.time()\n",
    "            \n",
    "            print('\\n\\033[44mStart training:\\033[0m')\n",
    "            display(loss_widget)   \n",
    "            last_eval_step = 1\n",
    "            while not sess.should_stop(): \n",
    "                        \n",
    "                # Train\n",
    "                global_step_, full_loss_, _ = sess.run([global_step, full_loss, train_op])\n",
    "                \n",
    "                # Evaluate\n",
    "                if (multistage_configuration[\"save_evaluation_steps\"] is not None and (global_step_ > 1)\n",
    "                    and global_step_  % multistage_configuration[\"save_evaluation_steps\"] == 0):\n",
    "                    num_epochs = multistage_configuration[\"test_num_iters_per_epoch\"]\n",
    "                    sess.run(clear_metrics_op)\n",
    "                    for epoch in range(num_epochs):\n",
    "                        viz.display_eval(loss_widget, global_step_, epoch + 1, num_epochs, start_time)\n",
    "                        sess.run(update_metrics_op) \n",
    "                        if epoch == num_epochs - 1: eval_summary = sess.run(eval_summary_op)\n",
    "                    # Write summary\n",
    "                    summary_writer.add_summary(eval_summary, global_step_)\n",
    "                    summary_writer.flush()\n",
    "                    \n",
    "                # Display\n",
    "                if (global_step_ - 1) % 20 == 0:\n",
    "                    viz.display_loss(loss_widget, global_step_, full_loss_, start_time,\n",
    "                                     multistage_configuration[\"train_num_samples_per_iter\"],\n",
    "                                     multistage_configuration[\"train_num_samples\"])\n",
    "                \n",
    "    except KeyboardInterrupt:\n",
    "        print('\\nInterrupted at step %d' % global_step_)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {
    "bea7b664d55547eeb27eef7df2421f86": {
     "views": [
      {
       "cell_index": 10
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
